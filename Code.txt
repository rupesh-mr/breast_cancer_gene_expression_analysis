!pip install kagglehub

import pandas as pd
from sklearn.preprocessing import StandardScaler
import kagglehub
from kagglehub import KaggleDatasetAdapter
file_path = "Breast_GSE45827.csv"
df = kagglehub.load_dataset(
  KaggleDatasetAdapter.PANDAS,
  "brunogrisci/breast-cancer-gene-expression-cumida",
  file_path
)

df=df.set_index("samples")

missing_values_per_column = df.isnull().sum()
total_missing_values = df.isnull().sum().sum()

print("Number of missing values in each attribute:")
print(missing_values_per_column)

print("\n Total number of missing values in the file:", total_missing_values)

duplicates = df[df.duplicated()]
print(f"Number of duplicate rows: {duplicates.shape[0]}")

num_cols = df.select_dtypes(include=['number']).columns
for col in num_cols:
    if (df[col] < 0).any():
        print(f"Column '{col}' contains negative values.")

from sklearn.preprocessing import OrdinalEncoder
column = "type"

# Initialize and fit the encoder
encoder = OrdinalEncoder()
df[[column]] = encoder.fit_transform(df[[column]])

df[column] = df[column].astype(int)
labels=df["type"]
encoded_file_path = "Breast_GSE45827_encoded.csv"
df.to_csv(encoded_file_path, index=True)
df.head()

encoding_map = {i: category for i, category in enumerate(encoder.categories_[0])}
print("Encoding Mapping:", encoding_map)

from sklearn.feature_selection import SelectKBest, f_classif

# X=pd.DataFrame(X_s, columns=df.columns, index=df.index)
skb = SelectKBest(f_classif, k=10)
X_sel = skb.fit_transform(df[df.columns[1:]], labels)
mask = skb.get_support()
selected_features = df.columns[1:][mask]
print(selected_features)

expr=df
labels=df["type"]
expr=expr.iloc[:,1:]
expr=expr.T
var = expr.var(axis=1)
expr_filt = expr.loc[var > var.quantile(0.5)]
print(f"Kept {expr_filt.shape[0]} high‑variance genes")

scaler = StandardScaler()
X_scaled = scaler.fit_transform(expr_filt.T)

X_scaled.shape

from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
pca = PCA()
pca1 = pca.fit_transform(X_scaled)

eigenvalues = pca.explained_variance_
eigenvectors = pca.components_

plt.figure(figsize=(26, 6))
plt.plot(range(1, len(eigenvalues) + 1), eigenvalues, marker='o')
plt.title('Eigenvalues in Descending Order')
plt.xlabel('Principal Component')
plt.ylabel('Eigenvalue')
plt.grid(True)
plt.show()

from sklearn.decomposition import PCA
import numpy as np

pca = PCA().fit(X_scaled)
cumulative_variance = pca.explained_variance_ratio_.cumsum()  # Cumulative sum of explained variance
num_components = np.argmax(cumulative_variance >= 0.95) + 1  # First index where variance reaches 95%
print(f"Optimal number of components: {num_components}")

num_components = np.argmax(cumulative_variance >= 0.85) + 1  # First index where variance reaches 85%
print(f"Optimal number of components: {num_components}")

import matplotlib.pyplot as plt

plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o')
plt.xlabel("Number of Components")
plt.ylabel("Cumulative Explained Variance")
plt.title("Scree Plot")
plt.show()

from sklearn.decomposition import PCA


pca = PCA(n_components=74, svd_solver="full", random_state=0)
X_pca_full = pca.fit_transform(X_scaled)
print(f"PCA retained {X_pca_full.shape[1]} components")

# For 3D plotting:
X_pca3 = X_pca_full[:, :3]

import plotly.express as px
df1 = pd.DataFrame(X_pca3, columns=["PC1","PC2","PC3"])
df1["cluster"] = labels.reset_index(drop=True)
fig = px.scatter_3d(df1, x="PC1", y="PC2", z="PC3",
                    color="cluster", opacity=0.8,
                    title="Original Clusters on PCA Space")
fig.show()

import umap
X_umap = umap.UMAP(n_components=5, random_state=0).fit_transform(X_pca_full)

X_umap.shape

from sklearn.cluster import KMeans
from sklearn.cluster import AgglomerativeClustering
from sklearn.mixture import GaussianMixture

kmeans = KMeans(n_clusters=6,random_state=42).fit(X_umap)
km_labels1 = kmeans.labels_
df2 = pd.DataFrame(X_pca3, columns=["PC1","PC2","PC3"])
df2["cluster"] = km_labels1
fig = px.scatter_3d(df2, x="PC1", y="PC2", z="PC3",
                    color="cluster", opacity=0.7,
                    title="Kmeans Clusters(k=6) on PCA Space")
fig.show()

from sklearn.metrics import silhouette_score

best_k = None
best_score = -1
for k in range(3, 9):
    kmeans = KMeans(n_clusters=k, random_state=42)
    y_kmeans = kmeans.fit_predict(X_umap)
    score = silhouette_score(X_umap, y_kmeans)
    print(f"K={k}, Silhouette Score: {score:.4f}")

    if score > best_score:
        best_score = score
        best_k = k

print(f"Best K: {best_k} with Silhouette Score: {best_score:.4f}")

kmeans = KMeans(n_clusters=3,random_state=42).fit(X_umap)
km_labels2 = kmeans.labels_
df2 = pd.DataFrame(X_pca3, columns=["PC1","PC2","PC3"])
df2["cluster"] = km_labels2
fig = px.scatter_3d(df2, x="PC1", y="PC2", z="PC3",
                    color="cluster", opacity=0.7,
                    title="Kmeans Clusters(k=3) on PCA Space")
fig.show()

from scipy.spatial.distance import mahalanobis
from sklearn.metrics import silhouette_score, adjusted_rand_score

# Mahalanobis KMeans Function
def kmeans_mahalanobis(X, k, max_iters=100):
    centroids = X[np.random.choice(len(X), k, replace=False)]
    VI = np.linalg.inv(np.cov(X.T))

    for _ in range(max_iters):
        labels = np.array([
            np.argmin([mahalanobis(x, centroid, VI) for centroid in centroids])
            for x in X
        ])
        new_centroids = np.array([
            X[labels == j].mean(axis=0) if np.any(labels == j) else centroids[j]
            for j in range(k)
        ])
        if np.allclose(centroids, new_centroids):
            break
        centroids = new_centroids

    return labels, centroids



labels_maha1, centroids_maha = kmeans_mahalanobis(X_umap, 6)
df1 = pd.DataFrame(X_pca3, columns=["PC1","PC2","PC3"])
df1["cluster"] = labels_maha1
fig = px.scatter_3d(df1, x="PC1", y="PC2", z="PC3",
                    color="cluster", opacity=0.7,
                    title="Kmeans Clusters(k=6) with mahalnobis distance on PCA Space")
fig.show()

best_k = None
best_score = -1
for k in range(3, 9):
    y_kmeans_maha,_ = kmeans_mahalanobis(X_umap,k)
    score = silhouette_score(X_umap, y_kmeans_maha)
    print(f"K={k}, Silhouette Score: {score:.4f}")

    if score > best_score:
        best_score = score
        best_k = k
        labels_maha2 = y_kmeans_maha

print(f"Best K: {best_k} with Silhouette Score: {best_score:.4f}")



df1 = pd.DataFrame(X_pca3, columns=["PC1","PC2","PC3"])
df1["cluster"] = labels_maha2
fig = px.scatter_3d(df1, x="PC1", y="PC2", z="PC3",
                    color="cluster", opacity=0.7,
                    title="Kmeans Clusters(k=4) with mahalnobis distance on PCA Space")
fig.show()

gmm = GaussianMixture(n_components=6, random_state=42).fit(X_umap)
gmm_labels1 = gmm.predict(X_umap)
df1 = pd.DataFrame(X_pca3, columns=["PC1","PC2","PC3"])
df1["cluster"] = gmm_labels1
fig = px.scatter_3d(df1, x="PC1", y="PC2", z="PC3",
                    color="cluster", opacity=0.7,
                    title="GMM Clusters(k=6) on PCA Space")
fig.show()

best_k = None
best_score = -1
for k in range(3, 9):
    gmm = GaussianMixture(n_components=k, random_state=42)
    y_gmm = gmm.fit_predict(X_umap)
    score = silhouette_score(X_umap, y_gmm)
    print(f"K={k}, Silhouette Score: {score:.4f}")

    if score > best_score:
        best_score = score
        best_k = k

print(f"Best K: {best_k} with Silhouette Score: {best_score:.4f}")

gmm = GaussianMixture(n_components=3, random_state=42).fit(X_umap)
gmm_labels2 = gmm.predict(X_umap)
df1 = pd.DataFrame(X_pca3, columns=["PC1","PC2","PC3"])
df1["cluster"] = gmm_labels2
fig = px.scatter_3d(df1, x="PC1", y="PC2", z="PC3",
                    color="cluster", opacity=0.7,
                    title="GMM Clusters(k=3) on PCA Space")
fig.show()

from sklearn.neighbors import NearestNeighbors
import numpy as np

# Find the optimal eps value using K-distance plot
neighbors = NearestNeighbors(n_neighbors=5)
neighbors_fit = neighbors.fit(X_scaled)
distances, indices = neighbors_fit.kneighbors(X_scaled)

# Sort and plot the distances (Elbow method)
distances = np.sort(distances[:, 4])
plt.figure(figsize=(8, 5))
plt.plot(distances)
plt.xlabel("Data Points Sorted by Distance")
plt.ylabel("5th Nearest Neighbor Distance")
plt.title("Elbow Method for Optimal Epsilon")
plt.grid(True)
plt.show()

from sklearn.cluster import DBSCAN

optimal_eps = 200
min_samples = 5

# Train DBSCAN
dbscan = DBSCAN(eps=optimal_eps, min_samples=min_samples)
y_dbscan1 = dbscan.fit_predict(X_scaled)

# Count clusters
num_clusters = len(set(y_dbscan1)) - (1 if -1 in y_dbscan1 else 0)
print(f"DBSCAN found {num_clusters} clusters")

# Evaluate only if DBSCAN found clusters
if num_clusters <= 1:
    print("DBSCAN did not find valid clusters.")

df1 = pd.DataFrame(X_pca3, columns=["PC1","PC2","PC3"])
df1["cluster"] = y_dbscan1
fig = px.scatter_3d(df1, x="PC1", y="PC2", z="PC3",
                    color="cluster", opacity=0.7,
                    title="DBSCAN Clusters on PCA Space")
fig.show()

from sklearn.metrics.pairwise import cosine_distances

#Cosine distance (better for high‑dimensional gene data)
D = cosine_distances(X_umap)

from sklearn.cluster import AgglomerativeClustering
agglo = AgglomerativeClustering(n_clusters=6,metric='precomputed',linkage='average').fit(D)
agglo_labels1 = agglo.labels_
df1 = pd.DataFrame(X_pca3, columns=["PC1","PC2","PC3"])
df1["cluster"] = agglo_labels1
fig = px.scatter_3d(df1, x="PC1", y="PC2", z="PC3",
                    color="cluster", opacity=0.7,
                    title="AGNES Clusters(k=6) on PCA Space")
fig.show()

from scipy.cluster.hierarchy import linkage, dendrogram
from scipy.spatial.distance import squareform
condensed_dist_matrix = squareform(D)
linkage_matrix = linkage(condensed_dist_matrix, method='average')
plt.figure(figsize=(10, 7))
dendrogram(linkage_matrix)
plt.title('Hierarchical Clustering Dendrogram (Cosine Distance)')
plt.xlabel('Sample Index')
plt.ylabel('Cosine Distance')
plt.show()

best_k = None
best_score = -1
for k in range(3, 9):
    agglo = AgglomerativeClustering(n_clusters=k)
    y_agglo = agglo.fit_predict(X_umap)
    score = silhouette_score(D, y_agglo,metric="precomputed")
    print(f"K={k}, Silhouette Score: {score:.4f}")

    if score > best_score:
        best_score = score
        best_k = k

print(f"Best K: {best_k} with Silhouette Score: {best_score:.4f}")

agglo = AgglomerativeClustering(n_clusters=4,metric='precomputed',linkage='average').fit(D)
agglo_labels2 = agglo.labels_
df1 = pd.DataFrame(X_pca3, columns=["PC1","PC2","PC3"])
df1["cluster"] = agglo_labels2
fig = px.scatter_3d(df1, x="PC1", y="PC2", z="PC3",
                    color="cluster", opacity=0.7,
                    title="AGNES Clusters(k=4) on PCA Space")
fig.show()

!pip install scikit-fuzzy

import numpy as np
import skfuzzy as fuzz


data = X_umap.T

n_clusters = 6
cntr, u, u0, d, jm, p, fpc = fuzz.cluster.cmeans(
    data, c=n_clusters, m=2.0, error=0.005, maxiter=1000, init=None, seed=0
)

fcm_labels1 = np.argmax(u, axis=0)
df1 = pd.DataFrame(X_pca3, columns=["PC1","PC2","PC3"])
df1["cluster"] = fcm_labels1
fig = px.scatter_3d(df1, x="PC1", y="PC2", z="PC3",
                    color="cluster", opacity=0.7,
                    title="FCMEANS Clusters(k=6) on PCA Space")
fig.show()

best_k = None
best_score = -1
for k in range(3, 9):
    n_clusters = k
    cntr, u, u0, d, jm, p, fpc = fuzz.cluster.cmeans(
    data, c=n_clusters, m=2.0, error=0.005, maxiter=1000, init=None, seed=0
)

    fcm_labels = np.argmax(u, axis=0)
    score = silhouette_score(X_umap, fcm_labels)
    print(f"K={k}, Silhouette Score: {score:.4f}")

    if score > best_score:
        best_score = score
        best_k = k

print(f"Best K: {best_k} with Silhouette Score: {best_score:.4f}")

n_clusters = 3
cntr, u, u0, d, jm, p, fpc = fuzz.cluster.cmeans(
    data, c=n_clusters, m=2.0, error=0.005, maxiter=1000, init=None, seed=0
)

fcm_labels2 = np.argmax(u, axis=0)
df1 = pd.DataFrame(X_pca3, columns=["PC1","PC2","PC3"])
df1["cluster"] = fcm_labels2
fig = px.scatter_3d(df1, x="PC1", y="PC2", z="PC3",
                    color="cluster", opacity=0.7,
                    title="FCMEANS Clusters(k=3) on PCA Space")
fig.show()

labels=df["type"]

from sklearn.metrics import adjusted_rand_score, silhouette_score
import numpy as np

def purity_score(y_true, y_pred):
    contingency = pd.crosstab(y_true, y_pred)
    return np.sum(np.max(contingency.values, axis=0)) / np.sum(contingency.values)

methods = {
    "KMeans": km_labels1,
    "Mahalanobis": labels_maha1,
    "GMM": gmm_labels1,
    "DBSCAN": y_dbscan1,
    "FCM": fcm_labels1,
}

results = []
for name, pred in methods.items():
    sil = silhouette_score(X_umap, pred)
    ari = adjusted_rand_score(labels, pred)
    pur = purity_score(labels, pred)
    results.append((name, ari, sil, pur))
sil_agglo=silhouette_score(D, agglo_labels1,metric="precomputed")
ari_agglo=adjusted_rand_score(labels, agglo_labels1)
pur_agglo=purity_score(labels, agglo_labels1)
results.append(("Agglomerative", ari_agglo, sil_agglo, pur_agglo))
results_df = pd.DataFrame(results, columns=["Method","ARI","Silhouette","Purity"])
print(results_df)

methods = {
    "KMeans": km_labels2,
    "Mahalanobis": labels_maha2,
    "GMM": gmm_labels2,
    "DBSCAN": y_dbscan1,
    "FCM": fcm_labels2,
}

results = []
for name, pred in methods.items():
    sil = silhouette_score(X_umap, pred)
    ari = adjusted_rand_score(labels, pred)
    pur = purity_score(labels, pred)
    results.append((name, ari, sil, pur))
sil_agglo=silhouette_score(D, agglo_labels2,metric="precomputed")
ari_agglo=adjusted_rand_score(labels, agglo_labels2)
pur_agglo=purity_score(labels, agglo_labels2)
results.append(("Agglomerative", ari_agglo, sil_agglo, pur_agglo))
results_df = pd.DataFrame(results, columns=["Method","ARI","Silhouette","Purity"])
print(results_df)

import numpy as np
import pandas as pd
from scipy.stats import ttest_ind

biomarkers = {}
for k in np.unique(labels):
    idx_in  = labels == k
    idx_out = ~idx_in

    # Perform t‐tests across genes:
    # expr_filt is genes × samples, so transpose to samples × genes
    data_in  = expr_filt.T[idx_in]
    data_out = expr_filt.T[idx_out]

    # ttest_ind returns (tstats, pvalues)
    tstats, pvals = ttest_ind(data_in, data_out, axis=0, equal_var=False)

    # Select genes with p < 0.01
    sig_idx = np.where(pvals < 0.01)[0]
    # Rank by absolute t‐statistic
    top_idx = sig_idx[np.argsort(-np.abs(tstats[sig_idx]))]
    top_genes = expr_filt.index[top_idx][:5]  # top 5 genes

    biomarkers[k] = list(top_genes)

for k, genes in biomarkers.items():
    print(f"Cluster {k} biomarkers:", genes)


import seaborn as sns
import matplotlib.pyplot as plt
from scipy.cluster.hierarchy import linkage


top25= expr_filt.var(axis=1).sort_values(ascending=False).index[:25]
data_hm = expr_filt.loc[top25].T.copy()
data_hm['cluster'] = agglo_labels1
data_hm_sorted = data_hm.sort_values('cluster')
heatmap_data = data_hm_sorted.drop('cluster', axis=1)
row_linkage = linkage(heatmap_data, method='average')


sns.clustermap(
    heatmap_data,
    row_linkage=row_linkage,
    col_cluster=False,
    cmap='vlag',
    figsize=(8, 10)
)
plt.title("Heatmap of Top 25 Variable Genes (Samples Ordered by Agglomerative Cluster)")
plt.show()



